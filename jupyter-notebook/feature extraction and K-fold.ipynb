{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import jieba.posseg as psg    # posseg可标注词语的词性\n",
    "from cnradical import Radical, RunOption    # cnradical工具包可获取一个字的偏旁部首和拼音\n",
    "import shutil\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/25405/Desktop/KG/train'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BIO标注及特征提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BIO标注\n",
    "files = os.listdir(train_dir)\n",
    "files = list(set([file.split('.')[0] for file in files]))\n",
    "for file in files:\n",
    "    path = os.path.join(train_dir, file + '.txt')\n",
    "    tag_path = os.path.join(train_dir, file + '.ann')\n",
    "    with open(path, 'r', encoding = 'utf8') as f:\n",
    "        texts = f.readlines()\n",
    "        texts_list = []\n",
    "        word_bounds = ['M' for s in texts for x in s]\n",
    "        word_flags = []\n",
    "        word_radicals = []\n",
    "        word_pinyins = []\n",
    "        radical = Radical(RunOption.Radical)    # 提取偏旁部首\n",
    "        pinyin = Radical(RunOption.Pinyin)    # 提取拼音\n",
    "        \n",
    "        for text in texts:\n",
    "            for word, flag in psg.cut(text):\n",
    "                if len(word) == 1:\n",
    "                    start = len(word_flags)\n",
    "                    word_bounds[start] = 'S'\n",
    "                    word_flags.append(flag)\n",
    "                else:\n",
    "                    start = len(word_flags)\n",
    "                    word_bounds[start] = 'B'\n",
    "                    word_flags += [flag] * len(word)\n",
    "                    end = len(word_flags) - 1\n",
    "                    word_bounds[end] = 'E'\n",
    "        \n",
    "        for s in texts:\n",
    "            for x in s:\n",
    "                texts_list.append(x)\n",
    "                if radical.trans_ch(x) is not None:\n",
    "                    word_radicals.append(radical.trans_ch(x))\n",
    "                else:\n",
    "                    word_radicals.append('UNK')\n",
    "                if pinyin.trans_ch(x) is not None:\n",
    "                    word_pinyins.append(pinyin.trans_ch(x))\n",
    "                else:\n",
    "                    word_pinyins.append('UNK')\n",
    "                    \n",
    "    with open(tag_path, 'r', encoding = 'utf8') as f2:\n",
    "        # 获取标签\n",
    "        tag_list = ['O' for s in texts for x in s]\n",
    "        tag = pd.read_csv(tag_path, header = None, sep = '\\t')\n",
    "        for i in range(tag.shape[0]):\n",
    "            tag_item = tag.iloc[i][1].split(' ')\n",
    "            clas, start, end = tag_item[0], int(tag_item[1]), int(tag_item[-1])\n",
    "            tag_list[start] = 'B-' + clas\n",
    "            for j in range(start + 1, end):\n",
    "                tag_list[j] = 'I-' + clas\n",
    "    \n",
    "    ff = open('C:/Users/25405/Desktop/KG/ttt/train3.txt', 'a', encoding = 'utf-8')\n",
    "    for i in range(len([x for s in texts for x in s])):\n",
    "        ff.write(texts_list[i]+' '+word_bounds[i]+' '+word_flags[i]+' '+word_radicals[i]+' '+word_pinyins[i]+' '+tag_list[i]+ '\\n')\n",
    "\n",
    "ff.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K折交叉验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/25405/Desktop/KG/ttt/train3.txt', 'r', encoding = 'utf-8') as ff3:\n",
    "    texts = ff3.readlines()\n",
    "#     print(len(texts)/10 * 7,len(texts)/10 * 9)\n",
    "#     train_texts = texts[0:245330]\n",
    "#     ver_texts = texts[245330:315501]\n",
    "#     test_texts = texts[315501:]\n",
    "    ar_texts = np.array(texts)\n",
    "    New_texts = KFold(n_splits = 5)\n",
    "    t = 1\n",
    "    for train_index, rest_index in New_texts.split(ar_texts):  # 对ar_texts数据建立5折交叉验证的划分\n",
    "        split_vt = KFold(n_splits = 2)\n",
    "        train_texts, rest_texts = ar_texts[train_index], ar_texts[rest_index]\n",
    "        for ver_index, test_index in split_vt.split(rest_texts):\n",
    "            ver_texts, test_texts = ar_texts[ver_index], ar_texts[test_index]\n",
    "        #print(train, train.shape, ver, ver.shape, test, test.shape)\n",
    "        fff = open('C:/Users/25405/Desktop/KG/ttt/train_' + str(t) + '.txt', 'w', encoding = 'utf-8')\n",
    "        fff2 = open('C:/Users/25405/Desktop/KG/ttt/var_' + str(t) + '.txt', 'w', encoding = 'utf-8')\n",
    "        fff3 = open('C:/Users/25405/Desktop/KG/ttt/test_' + str(t) + '.txt', 'w', encoding = 'utf-8')\n",
    "        fff.writelines(train_texts)\n",
    "        fff2.writelines(ver_texts)\n",
    "        fff3.writelines(test_texts)\n",
    "        t += 1\n",
    "        fff.close()\n",
    "        fff2.close()\n",
    "        fff3.close()\n",
    "\n",
    "ff3.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分句，取消空格对应的特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1,6):\n",
    "    f = codecs.open('C:/Users/25405/Desktop/KG/ttt/train_' + str(t) + '.txt', 'r', encoding = 'utf-8')\n",
    "    line = f.readline()\n",
    "    l_word = []\n",
    "    l_bound = []\n",
    "    l_flag = []\n",
    "    l_radical = []\n",
    "    l_pinyin = []\n",
    "    l_label = []\n",
    "    while line: \n",
    "        if line.split()[4] != 'O':    # 去除word为空的行\n",
    "            word = line.split()[0]\n",
    "            bound = line.split()[1]\n",
    "            flag = line.split()[2]\n",
    "            radical = line.split()[3]\n",
    "            pinyin = line.split()[4]\n",
    "            label = line.split()[5]\n",
    "            l_word.append(word)\n",
    "            l_bound.append(bound)\n",
    "            l_flag.append(flag)\n",
    "            l_radical.append(radical)\n",
    "            l_pinyin.append(pinyin)\n",
    "            l_label.append(label)\n",
    "            if word == \"。\":\n",
    "                l_word.append(\" \")\n",
    "                l_bound.append(\" \")\n",
    "                l_flag.append(\" \")\n",
    "                l_radical.append(\" \")\n",
    "                l_pinyin.append(\" \")\n",
    "                l_label.append(\" \")\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(l_word)):\n",
    "        f1 = open('C:/Users/25405/Desktop/KG/ttt/tvt/train_chinese_no_space_' + str(t) + '.txt', 'a', encoding = 'utf-8')\n",
    "        f1.write(l_word[i]+' '+l_bound[i]+' '+l_flag[i]+' '+l_radical[i]+' '+l_pinyin[i]+' '+l_label[i]+ '\\n')\n",
    "    f1.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 只保留BIO特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "for t in range(1,6):\n",
    "    f = codecs.open('C:/Users/25405/Desktop/KG/ttt/tvt/train_chinese_no_space_' + str(t) + '.txt', 'r', encoding = 'utf-8')\n",
    "    line = f.readline()\n",
    "    l_word = []\n",
    "    l_label = []\n",
    "    while line: \n",
    "        if line.strip() != \"\":\n",
    "            word = line.split()[0]\n",
    "            label = line.split()[5]\n",
    "            l_word.append(word)\n",
    "            l_label.append(label)\n",
    "        else:\n",
    "            l_word.append(\" \")\n",
    "            l_label.append(\" \")\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(l_word)):\n",
    "        f1 = open('C:/Users/25405/Desktop/KG/ttt/tvt_BIO/train_chinese_no_space_' + str(t) + '.txt', 'a', encoding = 'utf-8')\n",
    "        f1.write(l_word[i] + ' ' + l_label[i] + '\\n')\n",
    "    f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "for t in range(1,6):\n",
    "    f = codecs.open('C:/Users/25405/Desktop/KG/ttt/tvt/ver_chinese_no_space_' + str(t) + '.txt', 'r', encoding = 'utf-8')\n",
    "    line = f.readline()\n",
    "    l_word = []\n",
    "    l_label = []\n",
    "    while line: \n",
    "        if line.strip() != \"\":\n",
    "            word = line.split()[0]\n",
    "            label = line.split()[5]\n",
    "            l_word.append(word)\n",
    "            l_label.append(label)\n",
    "        else:\n",
    "            l_word.append(\" \")\n",
    "            l_label.append(\" \")\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(l_word)):\n",
    "        f1 = open('C:/Users/25405/Desktop/KG/ttt/tvt_BIO/ver_chinese_no_space_' + str(t) + '.txt', 'a', encoding = 'utf-8')\n",
    "        f1.write(l_word[i] + ' ' + l_label[i] + '\\n')\n",
    "    f1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(1,6):\n",
    "    f = codecs.open('C:/Users/25405/Desktop/KG/ttt/tvt/test_chinese_no_space_' + str(t) + '.txt', 'r', encoding = 'utf-8')\n",
    "    line = f.readline()\n",
    "    l_word = []\n",
    "    l_label = []\n",
    "    while line: \n",
    "        if line.strip() != \"\":    # strip()方法用于移除字符串头尾指定的字符（默认为空格或换行符）或字符序列\n",
    "            word = line.split()[0]\n",
    "            label = line.split()[5]\n",
    "            l_word.append(word)\n",
    "            l_label.append(label)\n",
    "        else:\n",
    "            l_word.append(\" \")\n",
    "            l_label.append(\" \")\n",
    "        line = f.readline()\n",
    "    f.close()\n",
    "\n",
    "    for i in range(len(l_word)):\n",
    "        f1 = open('C:/Users/25405/Desktop/KG/ttt/tvt_BIO/test_chinese_no_space_' + str(t) + '.txt', 'a', encoding = 'utf-8')\n",
    "        f1.write(l_word[i] + ' ' + l_label[i] + '\\n')\n",
    "    f1.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
