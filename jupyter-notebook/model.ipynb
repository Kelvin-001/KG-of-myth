{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib.crf import crf_log_likelihood, viterbi_decode    # 条件随机场的对数似然，维特比解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def network(inputs, shapes, num_tags, lstm_dim = 100, initializer = tf.truncated_normal_initializer()):\n",
    "    \"\"\"\n",
    "    接收一个批次样本的特征数据，计算出网络的输出值\n",
    "    :param char:type of int,id of chars  a tensor of shape 2-D [None, None]\n",
    "    bound, flag, radical, pinyin同上\n",
    "    \"\"\"\n",
    "    # - - - - - - - - - - - - - -特征嵌入- - - - - - - - - - - - - - - -\n",
    "    #将所有特征的id转换成一个固定长度的向量\n",
    "    embedding = []\n",
    "    keys = list(shapes.keys())\n",
    "    for key in keys:\n",
    "        # tf.name_scope()、tf.variable_scope()是两个作用域函数，主要用于变量共享\n",
    "            lookup = tf.get_variable(\n",
    "                name = key + '_embedding',\n",
    "                shape = shapes[key],\n",
    "                initializer = initializer\n",
    "            )\n",
    "            embedding.append(tf.nn.embedding_lookup(lookup, inputs[key]))\n",
    "    embed = tf.concat(embedding, axis = -1)\n",
    "    \n",
    "    sign = tf.sign(tf.abs(inputs[keys[0]]))\n",
    "    lengths = tf.reduce_sum(sign, reduction_indices = 1)\n",
    "    \n",
    "    # - - - - - - - - - - - - - -循环神经网络编码- - - - - - - - - - - - - - - -\n",
    "    with tf.variable_scope('BiLstm_layer1'):\n",
    "        lstm_cell = {}\n",
    "        for name in ['forward', 'backward']:\n",
    "            with tf.variable_scope(name):\n",
    "                lstm_cell[name] = rnn.BasicLSTMCell(\n",
    "                    lstm_dim\n",
    "                )\n",
    "        outputs1, final_states1 = tf.nn.bidirectional_dynamic_rnn(\n",
    "            lstm_cell['forward'],\n",
    "            lstm_cell['backward'],\n",
    "            embed,\n",
    "            dtype = tf.float32,\n",
    "            sequence_length = lengths\n",
    "        )\n",
    "    output1 = tf.concat(outputs1, axis = -1)\n",
    "    \n",
    "    with tf.variable_scope('BiLstm_layer2'):\n",
    "        lstm_cell = {}\n",
    "        for name in ['forward', 'backward']:\n",
    "            with tf.variable_scope(name):\n",
    "                lstm_cell[name] = rnn.BasicLSTMCell(\n",
    "                    lstm_dim\n",
    "                )\n",
    "        outputs, final_states = tf.nn.bidirectional_dynamic_rnn(\n",
    "                # final_states是最终状态，包含前向和反向的c和h（c: 长时记忆，h: 短时记忆）\n",
    "            lstm_cell['forward'],\n",
    "            lstm_cell['backward'],\n",
    "            output1,\n",
    "            dtype = tf.float32,\n",
    "            sequence_length = lengths    # 每句话的实际长度\n",
    "        )\n",
    "    output = tf.concat(outputs, axis = -1)\n",
    "    num_time = tf.shape(inputs[keys[0]])[1]\n",
    "    \n",
    "    \n",
    "    # - - - - - - - - - - - - - -输出映射- - - - - - - - - - - - - - - -\n",
    "    output = tf.reshape(output, [-1, 2 * lstm_dim])\n",
    "    with tf.variable_scope('project_layer1'):\n",
    "        w = tf.get_variable(\n",
    "            name = 'w',\n",
    "            shape = [2 * lstm_dim, lstm_dim],\n",
    "            initializer = initializer\n",
    "        )\n",
    "        b = tf.get_variable(\n",
    "            name = 'b',\n",
    "            shape = [lstm_dim],\n",
    "            initializer = tf.zeros_initializer()\n",
    "        )\n",
    "        output = tf.nn.relu(tf.matmul(output, w) + b)\n",
    "        \n",
    "    with tf.variable_scope('project_layer2'):\n",
    "        w = tf.get_variable(\n",
    "            name = 'w',\n",
    "            shape = [lstm_dim, num_tags],\n",
    "            initializer = initializer\n",
    "        )\n",
    "        b = tf.get_variable(\n",
    "            name = 'b',\n",
    "            shape = [num_tags],\n",
    "            initializer = tf.zeros_initializer()\n",
    "        )\n",
    "        output = tf.matmul(output, w) + b\n",
    "    output = tf.reshape(output, [-1, num_time, num_tags])\n",
    "    \n",
    "    return output, lengths    # batch_size, max_length, num_tags\n",
    "    \n",
    "class Model(object):\n",
    "    \n",
    "    def __init__(self, dict, lr = 0.0001):\n",
    "        # - - - - -  -用到的参数值-- - - - - - - -\n",
    "        self.num_char = len(dict['word'][0])\n",
    "        self.num_bound = len(dict['bound'][0])\n",
    "        self.num_flag = len(dict['flag'][0])\n",
    "        self.num_radical = len(dict['radical'][0])\n",
    "        self.num_pinyin = len(dict['pinyin'][0])\n",
    "        self.num_tags = len(dict['label'][0])\n",
    "        self.char_dim = 100\n",
    "        self.bound_dim = 20\n",
    "        self.flag_dim = 50\n",
    "        self.radical_dim = 50\n",
    "        self.pinyin_dim = 50\n",
    "        self.lstm_dim = 100    # lstm的神经元个数\n",
    "        self.lr = lr    # 学习率\n",
    "        self.map = dict\n",
    "        \n",
    "        # - - - - - - - - - - - - - - - -定义接收数据的placeholder- - - - - - - - - - - - - - - - - - \n",
    "        # tf.placeholder(dtype, shape=None, name=None): tensorFlow中的占位符，用于传入外部数据\n",
    "        self.char_inputs = tf.placeholder(dtype = tf.int32, shape = [None, None], name = 'char_inputs')\n",
    "        self.bound_inputs = tf.placeholder(dtype = tf.int32, shape = [None, None], name = 'bound_inputs')\n",
    "        self.flag_inputs = tf.placeholder(dtype = tf.int32, shape = [None, None], name = 'flag_inputs')\n",
    "        self.radical_inputs = tf.placeholder(dtype = tf.int32, shape = [None, None], name = 'radical_inputs')\n",
    "        self.pinyin_inputs = tf.placeholder(dtype = tf.int32, shape = [None, None], name = 'pinyin_inputs')\n",
    "        self.targets = tf.placeholder(dtype = tf.int32, shape = [None, None], name = 'targets')    # 真实值\n",
    "        self.global_step = tf.Variable(0, trainable = False)\n",
    "        \n",
    "        # - - - - - - - - - - - -计算模型输出值-- - - - - - - - - - - - - \n",
    "        self.logits, self.lengths = self.get_logits(self.char_inputs,\n",
    "                                                    self.bound_inputs,\n",
    "                                                    self.flag_inputs,\n",
    "                                                    self.radical_inputs,\n",
    "                                                    self.pinyin_inputs\n",
    "                                                    )\n",
    "        # - - - - - - - - - - - - - - - - -计算损失- - - - - - - - - - - - - - - - - - \n",
    "        self.cost = self.loss(self.logits, self.targets, self.lengths)\n",
    "        \n",
    "        # - - - - - - - - - - - -优化器优化-- - - - - - - - - - - - - \n",
    "        # 采用梯度截断技术\n",
    "        with tf.variable_scope('optimizer'):\n",
    "            opt = tf.train.AdamOptimizer(self.lr)\n",
    "            grad_vars = opt.compute_gradients(self.cost)\n",
    "            clip_grad_vars = [[tf.clip_by_value(g, -5, 5), v] for g, v in grad_vars]\n",
    "                # tf.clip_by_value():可以将一个张量中的数值限制在一个范围之内\n",
    "            self.train_op = opt.apply_gradients(clip_grad_vars, self.global_step)\n",
    "        self.saver = tf.train.Saver(tf.global_variables(), max_to_keep = 5)\n",
    "        \n",
    "    \n",
    "    def get_logits(self, char, bound, flag, radical, pinyin):    # 获取模型的输出值\n",
    "        \"\"\"\n",
    "        :param char:type of int,id of chars  a tensor of shape 2-D [None, None]\n",
    "        bound, flag, radical, pinyin同上\n",
    "        :return:3-d tensor [batch_size, max_length, num_tags]\n",
    "        \"\"\"\n",
    "        shapes = {}\n",
    "        shapes['char'] = [self.num_char, self.char_dim]\n",
    "        shapes['bound'] = [self.num_bound, self.bound_dim]\n",
    "        shapes['flag'] = [self.num_flag, self.flag_dim]\n",
    "        shapes['radical'] = [self.num_radical, self.radical_dim]\n",
    "        shapes['pinyin'] = [self.num_pinyin, self.pinyin_dim]\n",
    "        inputs = {}\n",
    "        inputs['char'] = char\n",
    "        inputs['bound'] = bound\n",
    "        inputs['flag'] = flag\n",
    "        inputs['radical'] = radical\n",
    "        inputs['pinyin'] = pinyin\n",
    "        \n",
    "        return network(inputs, shapes, lstm_dim = self.lstm_dim, num_tags = self.num_tags)\n",
    "    \n",
    "    def loss(self, output, targets, lengths):\n",
    "        batch_size = tf.shape(lengths)[0]\n",
    "        num_steps = tf.shape(output)[1]\n",
    "        with tf.variable_scope('crf_loss'):\n",
    "            small = -1000.0\n",
    "            start_logits = tf.concat(\n",
    "                [small * tf.ones(shape = [batch_size, 1, self.num_tags]), tf.zeros(shape = [batch_size, 1, 1])], axis = -1\n",
    "            )\n",
    "            pad_logits = tf.cast(small * tf.ones([batch_size, num_steps, 1]), tf.float32)\n",
    "            logits = tf.concat([output, pad_logits], axis = -1)\n",
    "            logits = tf.concat([start_logits, logits], axis = 1)\n",
    "            targets = tf.concat(\n",
    "                [tf.cast(self.num_tags * tf.ones([batch_size, 1]), tf.int32), targets], axis = -1\n",
    "            )\n",
    "            self.trans = tf.get_variable(\n",
    "                name = 'trans',\n",
    "                shape = [self.num_tags + 1, self.num_tags + 1],\n",
    "                initializer = tf.truncated_normal_initializer()\n",
    "            )\n",
    "            log_likelihood, self.trans = crf_log_likelihood(\n",
    "                inputs = logits, \n",
    "                tag_indices = targets,\n",
    "                transition_params = self.trans,\n",
    "                sequence_lengths = lengths\n",
    "            )\n",
    "            return tf.reduce_mean(-log_likelihood)\n",
    "        \n",
    "    def run_step(self, sess, batch, istrain = True):\n",
    "        if istrain:\n",
    "            feed_dict = {\n",
    "                self.char_inputs: batch[0],\n",
    "                self.bound_inputs: batch[1],\n",
    "                self.flag_inputs: batch[2],\n",
    "                self.radical_inputs: batch[3],\n",
    "                self.pinyin_inputs: batch[4],\n",
    "                self.targets: batch[5]\n",
    "            }\n",
    "            _, loss = sess.run([self.train_op, self.cost], feed_dict = feed_dict)\n",
    "            return loss\n",
    "        else:\n",
    "            feed_dict = {\n",
    "                self.char_inputs: batch[0],\n",
    "                self.bound_inputs: batch[1],\n",
    "                self.flag_inputs: batch[2],\n",
    "                self.radical_inputs: batch[3],\n",
    "                self.pinyin_inputs: batch[4]\n",
    "            }\n",
    "            logits, lengths = sess.run([self.logits, self.lengths], feed_dict = feed_dict)\n",
    "            return logits, lengths\n",
    "        \n",
    "    def decode(self, logits, lengths, matrix):\n",
    "        paths = []\n",
    "        small = -1000.0\n",
    "        start = np.asarray([[small] * self.num_tags + [0]])\n",
    "        for score, length in zip(logits, lengths):\n",
    "            score = score[:length]\n",
    "            pad = small * np.ones([length, 1])\n",
    "            logit = np.concatenate([score, pad], axis = -1)\n",
    "            logit = np.concatenate([start, logit], axis = 0)\n",
    "            path, _ = viterbi_decode(logit, matrix)\n",
    "            \n",
    "            paths.append(path[1:])\n",
    "        return paths\n",
    "    \n",
    "    def predict(self, sess, batch):\n",
    "        results = []\n",
    "        matrix = self.trans.eval()    # eval()函数用来执行一个字符串表达式，并返回表达式的值\n",
    "        logits, lengths = self.run_step(sess, batch, istrain = False)\n",
    "        paths = self.decode(logits. lengths, matrix)\n",
    "        chars = batch[0]\n",
    "        for i in range(len(paths)):\n",
    "            length = lengths[i]\n",
    "            string = [self.map['word'][0][index] for index in chars[i][:length]]\n",
    "            tags = [self.map['label'][0][index] for index in paths[i]]\n",
    "            result = [k for k in zip(string, tags)]\n",
    "            results.append(result)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix\n",
    "# x\n",
    "result = tf.nn.embedding_lookup(matrix, x)\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "sess = tf.compat.v1.Session()\n",
    "print(sess.run(result))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
