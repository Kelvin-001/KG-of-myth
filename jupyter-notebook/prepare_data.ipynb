{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import Counter\n",
    "import jieba.posseg as psg    # posseg可标注词语的词性\n",
    "from cnradical import Radical, RunOption    # cnradical工具包可获取一个字的偏旁部首和拼音\n",
    "import shutil\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.__file__)\n",
    "import sys\n",
    "sys.path.append('C:/Users/25405/Desktop/KG')\n",
    "from data_process import split_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = 'C:/Users/25405/Desktop/KG/train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(idx, split_method = None, split_name = 'train'):\n",
    "    \"\"\"\n",
    "    读取文本,切割,打标记,提取特征\n",
    "    :param idx: 文件名\n",
    "    :param split_method: 切割文本的方法\n",
    "    :param split_name: 最终保存的文件夹名字\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    # 获取句子\n",
    "    if split_method is None:\n",
    "        with open(f'{train_dir}/{idx}.txt', 'r', encoding = 'utf-8') as f:\n",
    "            texts = f.readlines()\n",
    "    else:\n",
    "        with open(f'{train_dir}/{idx}.txt', 'r', encoding = 'utf-8') as f:\n",
    "            texts = f.read()\n",
    "            texts = split_method(texts) \n",
    "    data['word'] = texts\n",
    "    \n",
    "#     texts_list = []\n",
    "#     for s in texts:\n",
    "#         for x in s:\n",
    "#             texts_list.append(x)    \n",
    "    \n",
    "            \n",
    "    # 获取标签\n",
    "    tag_list = ['O' for s in texts for x in s]\n",
    "    tag = pd.read_csv(f'{train_dir}/{idx}.ann', header = None, sep = '\\t')\n",
    "    for i in range(tag.shape[0]):\n",
    "        tag_item = tag.iloc[i][1].split(' ')\n",
    "        clas, start, end = tag_item[0], int(tag_item[1]), int(tag_item[-1])\n",
    "        tag_list[start] = 'B-' + clas\n",
    "        for j in range(start + 1, end):\n",
    "            tag_list[j] = 'I-' + clas\n",
    "    assert len([x for s in texts for x in s]) == len(tag_list)\n",
    "    \n",
    "    # 提取词性和词边界特征\n",
    "    word_bounds = ['M' for element in tag_list]\n",
    "    word_flags = []    # 词性特征\n",
    "    for text in texts:\n",
    "        for word, flag in psg.cut(text):\n",
    "            if len(word) == 1:\n",
    "                start = len(word_flags)\n",
    "                word_bounds[start] = 'S'\n",
    "                word_flags.append(flag)\n",
    "            else:\n",
    "                start = len(word_flags)\n",
    "                word_bounds[start] = 'B'\n",
    "                word_flags += [flag] * len(word)\n",
    "                end = len(word_flags) - 1\n",
    "                word_bounds[end] = 'E'\n",
    "                \n",
    "    # 统一截断\n",
    "    tags = []\n",
    "    bounds = []\n",
    "    flags = []\n",
    "    start = 0\n",
    "    end = 0\n",
    "    for s in texts:\n",
    "        l = len(s)\n",
    "        end += l\n",
    "        tags.append(tag_list[start : end])\n",
    "        bounds.append(word_bounds[start : end])\n",
    "        flags.append(word_flags[start : end])\n",
    "        start += l\n",
    "    data['bound'] = bounds\n",
    "    data['flag'] = flags\n",
    "    data['label'] = tags\n",
    "    \n",
    "    # 获取偏旁部首和拼音特征\n",
    "    radical = Radical(RunOption.Radical)    # 提取偏旁部首\n",
    "    pinyin = Radical(RunOption.Pinyin)    # 提取拼音\n",
    "    # 没有偏旁部首、拼音的字标上UNK\n",
    "    data['radical'] = [[radical.trans_ch(x) if radical.trans_ch(x) is not None else 'UNK' for x in s] for s in texts]\n",
    "    data['pinyin'] = [[pinyin.trans_ch(x) if pinyin.trans_ch(x) is not None else 'UNK' for x in s] for s in texts]\n",
    "    \n",
    "    # 存储数据\n",
    "    num_samples = len(texts)\n",
    "    num_cols = len(data.keys())\n",
    "    \n",
    "    dataset = []\n",
    "    for i in range(num_samples):\n",
    "        # records = list(zip([list(v[i]) for v in data.values()]))    # 打包为元组的列表\n",
    "        records = list(zip(*[list(v[i]) for v in data.values()]))\n",
    "        dataset += records + [['sep'] * num_cols]\n",
    "    dataset = dataset[:-1]\n",
    "    dataset = pd.DataFrame(dataset, columns = data.keys())\n",
    "    save_path = f'C:/Users/25405/Desktop/KG/data/prepare/{split_name}/{idx}.txt'\n",
    "\n",
    "    def clean_word(w):\n",
    "        if w == '\\n':\n",
    "            return 'LB'\n",
    "        if w in [' ', '\\t', '\\u2003']:   # '\\u2003'为中文的空格\n",
    "            return 'SPACE'\n",
    "        if w.isdigit():\n",
    "            return 'num'\n",
    "        return w\n",
    "    dataset['word'] = dataset['word'].apply(clean_word)\n",
    "    dataset.to_csv(save_path, index = False, sep = ' ', encoding = 'utf-8')\n",
    "    #return texts[0], tags[0], bounds[0], flags[0], data['radical'][0], data['pinyin'][0]\n",
    "    \n",
    "def multi_process(split_method = None, train_ratio = 0.8):\n",
    "    if os.path.exists('C:/Users/25405/Desktop/KG/data/prepare/'):\n",
    "        shutil.rmtree('C:/Users/25405/Desktop/KG/data/prepare/')    # shutil.rmtree：递归的删除文件\n",
    "    if not os.path.exists('C:/Users/25405/Desktop/KG/data/prepare/train/'):\n",
    "        os.makedirs('C:/Users/25405/Desktop/KG/data/prepare/train')\n",
    "        os.makedirs('C:/Users/25405/Desktop/KG/data/prepare/test')\n",
    "    idxs = list(set([file.split('.')[0] for file in os.listdir(train_dir)]))\n",
    "    shuffle(idxs)\n",
    "    index = int(len(idxs) * train_ratio)    # 训练集的截止下标\n",
    "    train_ids = idxs[:index]    # 训练集文件名集合\n",
    "    test_ids = idxs[index:]    # 测试集文件名集合\n",
    "\n",
    "# #     import multiprocessing as mp    # 引入多进程，用线程池的方式来调用\n",
    "# #     num_cpus = mp.cpu_count()\n",
    "# #     pool = mp.Pool(num_cpus)\n",
    "# #     results = []\n",
    "# #     for idx in train_ids:\n",
    "# #         result = pool.apply_async(process_text, args = (idx, split_method, 'train'))    # 异步\n",
    "# #         results.append(result)\n",
    "# #     for idx in test_ids:\n",
    "# #         result = pool.apply_async(process_text, args = (idx, split_method, 'test'))\n",
    "# #         results.append(result)\n",
    "# #     pool.close()\n",
    "# #     pool.join()    # 主进程阻塞等待子进程的退出，join方法要在close或terminate之后使用\n",
    "# #     [r.get() for r in results]\n",
    "\n",
    "    results = []\n",
    "    for idx in train_ids:\n",
    "        result = process_text(*(idx, split_method, 'train'))\n",
    "        results.append(result)\n",
    "    for idx in test_ids:\n",
    "        result = process_text(*(idx, split_method, 'test'))\n",
    "        results.append(result)\n",
    "    [r for r in results]\n",
    "    \n",
    "def mapping(l_data, threshold = 10, is_word = False, sep = 'sep', is_label = False):    # 去掉sep\n",
    "    count = Counter(l_data)    # 统计每个数据出现的个数，返回一个字典\n",
    "    if sep is not None:\n",
    "        count.pop(sep)\n",
    "    if is_word:\n",
    "        count['PAD'] = 100000001\n",
    "        count['UNK'] = 100000000\n",
    "        l_data = sorted(count.items(), key = lambda x:x[1], reverse = True)\n",
    "        l_data = [x[0] for x in l_data if x[1] >= threshold]\n",
    "        # 将出现频率小的词设成Unknown\n",
    "        item = l_data\n",
    "        item2id = {item[i] : i for i in range(len(item))}\n",
    "    elif is_label:\n",
    "        l_data = sorted(count.items(), key = lambda x:x[1], reverse = True)\n",
    "        l_data = [x[0] for x in l_data]\n",
    "        item = l_data\n",
    "        item2id = {item[i] : i for i in range(len(item))}\n",
    "    else:\n",
    "        count['PAD'] = 100000001\n",
    "        l_data = sorted(count.items(), key = lambda x:x[1], reverse = True)\n",
    "        l_data = [x[0] for x in l_data]\n",
    "        item = l_data\n",
    "        item2id = {item[i] : i for i in range(len(item))}\n",
    "    return item, item2id\n",
    "    \n",
    "    \n",
    "def get_dict():\n",
    "    map_dict = {}\n",
    "    from glob import glob    # 遍历文件的一个工具（glob模块用来查找文件目录和文件，glob.glob()可同时获取所有的匹配路径）\n",
    "    all_word, all_bound, all_flag, all_label, all_radical, all_pinyin = [], [], [], [], [], []\n",
    "    for file in glob('C:/Users/25405/Desktop/KG/data/prepare/train/*.txt') + \\\n",
    "    glob('C:/Users/25405/Desktop/KG/data/prepare/test/*.txt'):\n",
    "        df = pd.read_csv(file, sep = ' ')\n",
    "        all_word += df['word'].tolist()\n",
    "        all_bound += df['bound'].tolist()\n",
    "        all_flag += df['flag'].tolist()\n",
    "        all_label += df['label'].tolist()\n",
    "        all_radical += df['radical'].tolist()\n",
    "        all_pinyin += df['pinyin'].tolist()\n",
    "    map_dict['word'] = mapping(all_word, threshold = 20, is_word = True)    # 返回的是一个元组\n",
    "    map_dict['bound'] = mapping(all_bound)\n",
    "    map_dict['flag'] = mapping(all_flag)\n",
    "    map_dict['label'] = mapping(all_label, is_label = True)\n",
    "    map_dict['radical'] = mapping(all_radical)\n",
    "    map_dict['pinyin'] = mapping(all_pinyin)\n",
    "    \n",
    "    with open(f'C:/Users/25405/Desktop/KG/data/prepare/dict.pkl', 'wb') as f:\n",
    "        pickle.dump(map_dict, f)    # 序列化对象\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    #print(process_text('0', split_method = split_text, split_name = 'train'))\n",
    "    #print(list(set([file.split('.')[0] for file in os.listdir(train_dir)])))\n",
    "    multi_process(split_text)\n",
    "    get_dict()\n",
    "#     with open(f'C:/Users/25405/Desktop/KG/data/prepare/dict.pkl', 'rb') as f:\n",
    "#         map_dict = pickle.load(f)\n",
    "#     print(map_dict['bound'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
